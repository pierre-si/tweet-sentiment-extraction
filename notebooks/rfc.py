#%%
import pickle

from pprint import pprint
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib
from matplotlib import pyplot as plt
import seaborn as sns

#%%
DATA_PATH = "../data/datasets/tfidf/"
df = pd.read_csv("../data/datasets/contents.csv", dtype={"sentiment": "category"})

with open(DATA_PATH + "features_train.pickle", "rb") as data:
    features_train = pickle.load(data)

with open(DATA_PATH + "labels_train.pickle", "rb") as data:
    labels_train = pickle.load(data)

with open(DATA_PATH + "features_test.pickle", "rb") as data:
    features_test = pickle.load(data)

with open(DATA_PATH + "labels_test.pickle", "rb") as data:
    labels_test = pickle.load(data)

#%%
features_train = features_train[:10_000]
labels_train = labels_train[:10_000]
# %%
rf_0 = RandomForestClassifier(random_state=0)

pprint(rf_0.get_params())
# %%
# Random Search
n_estimators = [int(x) for x in np.linspace(start=200, stop=1000, num=5)]
max_features = ["auto", "sqrt"]
max_depth = [int(x) for x in np.linspace(20, 100, num=5)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]

random_grid = {
    "n_estimators": n_estimators,
    "max_features": max_features,
    "max_depth": max_depth,
    "min_samples_split": min_samples_split,
    "min_samples_leaf": min_samples_leaf,
    "bootstrap": bootstrap,
}

pprint(random_grid)
# %%
# First create the base model to tune
rfc = RandomForestClassifier(random_state=8)

# Definition of the random search
random_search = RandomizedSearchCV(
    estimator=rfc,
    param_distributions=random_grid,
    n_iter=50,
    scoring="accuracy",
    cv=3,
    verbose=1,
    random_state=8,
    n_jobs=-1,
)

# Fit the random search model
random_search.fit(features_train, labels_train)
# %%
# Gridsearch around the best found by RandomSearch
# skipped. (not really necessary)
# %%
print("The best hyperparameters from Grid Search are:")
print(random_search.best_params_)
print("")
print("The mean accuracy of a model with these hyperparameters is:")
print(random_search.best_score_)
best_rfc = random_search.best_estimator_
# %%
# FIT
best_rfc.fit(features_train, labels_train)
# %%
rfc_pred = best_rfc.predict(features_test)
# %%
# Training accuracy
print("The training accuracy is: ")
print(accuracy_score(labels_train, best_rfc.predict(features_train)))
# %%
# Test accuracy
# 0.649
print("The test accuracy is: ")
print(accuracy_score(labels_test, rfc_pred))
# %%
# Classification report
print("Classification report")
print(classification_report(labels_test, rfc_pred))
# %%
aux_df = pd.DataFrame(
    [["bug", 0], ["feature", 1], ["question", 2]],
    columns=["sentiment", "sentiment_code"],
)


conf_matrix = confusion_matrix(labels_test, rfc_pred)
plt.figure(figsize=(12.8, 6))
sns.heatmap(
    conf_matrix,
    annot=True,
    xticklabels=aux_df["sentiment"].values,
    yticklabels=aux_df["sentiment"].values,
    cmap="Blues",
)
plt.ylabel("Predicted")
plt.xlabel("Actual")
plt.title("Confusion matrix")
plt.show()
# %%
# Default parameters RF
# Acc: 0.631
base_model = RandomForestClassifier(random_state=8)
base_model.fit(features_train, labels_train)
accuracy_score(labels_test, base_model.predict(features_test))
# %%
d = {
    "Model": "Random Forest",
    "Training Set Accuracy": accuracy_score(
        labels_train, best_rfc.predict(features_train)
    ),
    "Test Set Accuracy": accuracy_score(labels_test, rfc_pred),
}

df_models_rfc = pd.DataFrame(d, index=[0])
# %%
with open("../models/best_rfc.pickle", "wb") as output:
    pickle.dump(best_rfc, output)

with open("../models/df_models_rfc.pickle", "wb") as output:
    pickle.dump(df_models_rfc, output)

# %%
